{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Neural Mutual Information (MI) \n",
    "\n",
    "In this notebook we will show how to compute the Neural Mutual Information (NMI) between classes of data during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "import znnl as nl\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "\n",
    "from papyrus.measurements import (\n",
    "    Loss, Accuracy, NTKEntropy\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import jax\n",
    "jax.default_backend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the demo of ZnNL, we will reduce the number of data points used for training and computing the Mutual Informtaion\n",
    "To scale the computation, just increase the selected number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 100\n",
    "num_nmi_per_class = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generators\n",
    "\n",
    "For the sake of covereage, we will look at the NTK properties of the Fuel data set for a small model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = nl.data.MNISTGenerator(num_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Networks and Models\n",
    "\n",
    "Now we can define the network architectures for which we will compute the NTK of the data.\n",
    "\n",
    "The batch size defined in the model class refers to the batching in the NTK calculation. When calculating the NTK, the number of data points used in that calculation must be an integer mutliple of the batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple CNN module.\n",
    "    \"\"\"\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = x.reshape((x.shape[0], -1))  # flatten\n",
    "        x = nn.Dense(features=32)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=32)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=10)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nl.models.FlaxModel(\n",
    "    flax_module=DenseModule(),\n",
    "    optimizer=optax.sgd(learning_rate=0.005, momentum=0.9),\n",
    "    input_shape=(1, 28, 28, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recording \n",
    "\n",
    "We will record the loss and accuracy of the train and test data sets during training to see how well the model is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_recorder = nl.training_recording.JaxRecorder(\n",
    "    name=\"train_recorder\",\n",
    "    measurements=[\n",
    "        Loss(name=\"loss\", apply_fn=nl.loss_functions.CrossEntropyLoss()),\n",
    "        Accuracy(name=\"accuracy\", apply_fn=nl.accuracy_functions.LabelAccuracy()),\n",
    "    ],\n",
    "    storage_path=\".\",\n",
    "    update_rate=1, \n",
    "    chunk_size=1e5\n",
    ")\n",
    "train_recorder.instantiate_recorder(\n",
    "    data_set=data_generator.train_ds, \n",
    "    model=model\n",
    ")\n",
    "\n",
    "\n",
    "test_recorder = nl.training_recording.JaxRecorder(\n",
    "    name=\"test_recorder\",\n",
    "    measurements=[\n",
    "        Loss(name=\"loss\", apply_fn=nl.loss_functions.CrossEntropyLoss()),\n",
    "        Accuracy(name=\"accuracy\", apply_fn=nl.accuracy_functions.LabelAccuracy()),\n",
    "    ],\n",
    "    storage_path=\".\",\n",
    "    update_rate=1, \n",
    "    chunk_size=1e5\n",
    ")\n",
    "test_recorder.instantiate_recorder(\n",
    "    data_set=data_generator.test_ds, \n",
    "    model=model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Neural MI\n",
    "\n",
    "In order to compute the Neural MI, we will need to compute the von Neumann Entropy of the NTK. \n",
    "We will create a subset of the training data to compute the NTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = num_nmi_per_class\n",
    "\n",
    "mni_ds = {\"inputs\": [], \"targets\": []}\n",
    "\n",
    "for i in [0, 1, 8]:\n",
    "    idx = np.argmax(data_generator.train_ds['targets'], axis=1) == i\n",
    "    num_points = np.sum(idx)\n",
    "    print(f\"Number of data points for class {i}: {num_points} of which {n_points} will be selected\")\n",
    "\n",
    "    # Select the first n_points of class i\n",
    "    mni_ds[\"inputs\"].extend(data_generator.train_ds['inputs'][idx][:n_points])\n",
    "    mni_ds[\"targets\"].extend(data_generator.train_ds['targets'][idx][:n_points])\n",
    "\n",
    "mni_ds = {k: np.array(v) for k, v in mni_ds.items()}\n",
    "print(f\"Total number of data points to record the Mutual Information: {len(mni_ds['inputs'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mutual Information is a quantity that measures the amount of information that one distribution has about another. \n",
    "In our case, we are interested in the amount of information that that one class of data has about another.\n",
    "\n",
    "Since comparing all classes to all other classes is overly complicated, we will compare classes [0, 1, 8]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntk_combintaion_computation = nl.analysis.JAXNTKCombinations(\n",
    "    apply_fn=model.ntk_apply_fn, \n",
    "    class_labels=[0, 1, 8], # Selecting the classes to compute the Neural MI for\n",
    "    batch_size=10,\n",
    ")\n",
    "mni_recorder = nl.training_recording.JaxRecorder(\n",
    "    name=\"mni_recorder\",\n",
    "    measurements=[\n",
    "        NTKEntropy(name=\"ntk_entropy\", effective=False, normalize_eigenvalues=True),\n",
    "    ],\n",
    "    storage_path=\".\",\n",
    "    update_rate=1,\n",
    "    chunk_size=1e5\n",
    ")\n",
    "mni_recorder.instantiate_recorder(\n",
    "    data_set=mni_ds, \n",
    "    ntk_computation=ntk_combintaion_computation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = nl.training_strategies.SimpleTraining(\n",
    "    model=model, \n",
    "    loss_fn=nl.loss_functions.CrossEntropyLoss(),\n",
    "    recorders=[\n",
    "        train_recorder, \n",
    "        test_recorder, \n",
    "        mni_recorder\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_training_metrics = trainer.train_model(\n",
    "    train_ds=data_generator.train_ds, \n",
    "    test_ds=data_generator.test_ds,\n",
    "    batch_size=10,\n",
    "    epochs=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_report = train_recorder.gather()\n",
    "test_report = test_recorder.gather()\n",
    "mni_report = mni_recorder.gather()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "axs[0].plot(train_report[\"loss\"], label=\"train\")\n",
    "axs[0].plot(test_report[\"loss\"], label=\"test\")\n",
    "axs[0].set_yscale(\"log\")\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "\n",
    "axs[1].plot(train_report[\"accuracy\"], label=\"train\")\n",
    "axs[1].plot(test_report[\"accuracy\"], label=\"test\")\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "axs[1].set_ylabel(\"Accuracy\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Nerual MI\n",
    "\n",
    "*To obtain results in the following part, you need to uncomment the `nmi_recorder` in when defining the `trainer` object above.*\n",
    "\n",
    "\n",
    "Using the `JAXNTKCombinations` module, we obtain a set of entropy values for all the combinations of the classes. We can then compute the Neural MI using the entropy values.\n",
    "\n",
    "The mutual information two correlated subsystems is obtained by:\n",
    "\n",
    "$$I(X;Y) = S(X) + S(Y) - S(X,Y)$$\n",
    "\n",
    "where $S(X)$ is the entropy of the first subsystem, $S(Y)$ is the entropy of the second subsystem, and $S(X,Y)$ is the joint entropy of the two subsystems.\n",
    "Using this formula, we can compute the Mutual Information of the classes of the data.\n",
    "The value of $$I(X;Y)$$ will however, depend on the size of the entropy values. For that reason we will normalize the Mutual Information by the sum of the entropies of the two classes:\n",
    "\n",
    "$$I(X;Y) = \\frac{2 \\cdot I(X;Y)}{S(X) + S(Y)} \\in [0, 1]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies = np.array(mni_report['ntk_entropy'])\n",
    "\n",
    "print(f\"We obtain one entropie for each label combination: {entropies.shape}\")\n",
    "print(f\"The label combinations are: {ntk_combintaion_computation.label_combinations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mni_norm = {\n",
    "    \"I(0, 1)\": 2* ( entropies[:, 0] + entropies[:, 1] - entropies[:, 3]) / (entropies[:, 0] + entropies[:, 1]),\n",
    "    \"I(0, 8)\": 2* ( entropies[:, 0] + entropies[:, 2] - entropies[:, 4]) / (entropies[:, 0] + entropies[:, 2]),\n",
    "    \"I(1, 8)\": 2* ( entropies[:, 1] + entropies[:, 2] - entropies[:, 5]) / (entropies[:, 1] + entropies[:, 2]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4), tight_layout=True)\n",
    "\n",
    "# Plot the Entropies \n",
    "axs[0].plot(entropies[:, 0] , label=\"H(0)\")\n",
    "axs[0].plot(entropies[:, 1] , label=\"H(1)\")\n",
    "axs[0].plot(entropies[:, 2] , label=\"H(8)\")\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "axs[0].set_ylabel(\"Entropy\")\n",
    "axs[0].legend()\n",
    "\n",
    "for key, value in mni_norm.items():\n",
    "    axs[1].plot(value, label=key)\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "axs[1].set_ylabel(\"Normalized MI\")\n",
    "axs[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
