{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1155f508",
   "metadata": {},
   "source": [
    "# Computing Entropy\n",
    "\n",
    "In this notebook we will walkthrough entropy computations and some of the options associated with them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f5d943",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "For all entropy calculations we will use the neural tangent kernel. Therefore, we will exclusively be using the neural tangents library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02668461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-02 15:09:42.140673: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-02 15:09:45.639478: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/slurm/lib:/software/opt/focal/x86_64/spack/2021.12/spack/opt/spack/linux-ubuntu20.04-x86_64_v2/gcc-11.2.0/cudnn-8.2.4.15-11.4-r5srvd2bjed7zlr75cesfus3nwsjprw6/lib64:/software/opt/focal/x86_64/spack/2021.12/spack/opt/spack/linux-ubuntu20.04-x86_64_v2/gcc-11.2.0/cuda-11.4.2-jefqkwdwi245u5nbdg5tw3ufrucvsnag/lib64:/opt/slurm/lib:\n",
      "2023-01-02 15:09:45.639991: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/slurm/lib:/software/opt/focal/x86_64/spack/2021.12/spack/opt/spack/linux-ubuntu20.04-x86_64_v2/gcc-11.2.0/cudnn-8.2.4.15-11.4-r5srvd2bjed7zlr75cesfus3nwsjprw6/lib64:/software/opt/focal/x86_64/spack/2021.12/spack/opt/spack/linux-ubuntu20.04-x86_64_v2/gcc-11.2.0/cuda-11.4.2-jefqkwdwi245u5nbdg5tw3ufrucvsnag/lib64:/opt/slurm/lib:\n",
      "2023-01-02 15:09:45.640027: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-01-02 15:09:53.541520: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "/home/s/S.Tovey/miniconda3/envs/zincware/lib/python3.8/site-packages/chex/_src/pytypes.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead.\n",
      "  PyTreeDef = type(jax.tree_structure(None))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "import znrnd\n",
    "from neural_tangents import stax\n",
    "import optax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import jax\n",
    "jax.default_backend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d379c99",
   "metadata": {},
   "source": [
    "### Data generators\n",
    "\n",
    "For the sake of covereage, we will look at the entropy of the all the data generators on small networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d70094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset = pd.read_csv(url, names=column_names,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)\n",
    "\n",
    "dataset = raw_dataset.copy()\n",
    "dataset = dataset.dropna()\n",
    "dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "dataset = pd.get_dummies(dataset, columns=['Origin'], prefix='', prefix_sep='')\n",
    "\n",
    "\n",
    "dataset = (dataset-dataset.mean())/dataset.std()\n",
    "\n",
    "class MPGDataGenerator(znrnd.data.DataGenerator):\n",
    "    \"\"\"\n",
    "    Data generator for the MPG dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Constructor for the data generator.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset\n",
    "        \"\"\"        \n",
    "        train_ds = dataset.sample(frac=0.8, random_state=0)\n",
    "        train_labels = train_ds.pop(\"MPG\")\n",
    "        test_ds = dataset.drop(train_ds.index)\n",
    "        test_labels = test_ds.pop(\"MPG\")\n",
    "        \n",
    "        self.train_ds = {\"inputs\": train_ds.to_numpy(), \"targets\": train_labels.to_numpy().reshape(-1, 1)}\n",
    "        self.test_ds = {\"inputs\": test_ds.to_numpy(), \"targets\": test_labels.to_numpy().reshape(-1, 1)}\n",
    "        \n",
    "        self.data_pool = self.train_ds[\"inputs\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "088201ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuel_generator = MPGDataGenerator(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fa1b9d",
   "metadata": {},
   "source": [
    "###  Networks and Models\n",
    "\n",
    "Now we can define the network architectures for which we will compute the entropy of the data. Let's use a dense network for the fuel data and a convolutional network for the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62250ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_network = stax.serial(\n",
    "    stax.Dense(32),\n",
    "    stax.Relu(),\n",
    "    stax.Dense(32),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd7b67ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuel_model = znrnd.models.NTModel(\n",
    "    nt_module=dense_network,\n",
    "    optimizer=optax.adam(learning_rate=0.001),\n",
    "    loss_fn=znrnd.loss_functions.MeanPowerLoss(order=2),\n",
    "    input_shape=(9,),\n",
    "    training_threshold=0.001,\n",
    "    batch_size=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699ab687",
   "metadata": {},
   "source": [
    "### Computing the Entropy\n",
    "\n",
    "Let's compute the entropy of a small subset of the data points, let's say, 10 points from each. To do so, we will perform the following steps:\n",
    "\n",
    "1. Select the subset of data.\n",
    "2. Compute the NTK matrix for each model.\n",
    "3. Instantiate an entropy calculator for each matrix.\n",
    "4. Compute the entropy for each matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1f79abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "\n",
    "fuel_data = fuel_generator[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b17ba4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "\n",
    "fuel_ntk = fuel_model.compute_ntk(fuel_data, normalize=False)[\"empirical\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b5fb103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "\n",
    "fuel_calculator = znrnd.analysis.EntropyAnalysis(matrix=fuel_ntk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a19af94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4\n",
    "\n",
    "fuel_entropy = fuel_calculator.compute_von_neumann_entropy(\n",
    "    effective=True, normalize_eig=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2127463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73508817\n"
     ]
    }
   ],
   "source": [
    "print(fuel_entropy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
